{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>abu</th>\n",
       "      <th>according</th>\n",
       "      <th>achilles</th>\n",
       "      <th>acknowledged</th>\n",
       "      <th>action</th>\n",
       "      <th>added</th>\n",
       "      <th>admits</th>\n",
       "      <th>admitted</th>\n",
       "      <th>advantage</th>\n",
       "      <th>...</th>\n",
       "      <th>worse</th>\n",
       "      <th>worst</th>\n",
       "      <th>worth</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>youll</th>\n",
       "      <th>youve</th>\n",
       "      <th>yuki</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AlonsoPraisesAMR</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HamiltonsCarIssues</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeclercsPoorQualifying</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PerezOnHisFuture</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SainzOnHisFuture</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotoOnMercedes</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TsunodaPoints</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 802 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        able  abu  according  achilles  acknowledged  action  \\\n",
       "AlonsoPraisesAMR           0    0          0         0             1       1   \n",
       "HamiltonsCarIssues         1    0          0         0             1       0   \n",
       "LeclercsPoorQualifying     0    0          0         0             0       0   \n",
       "PerezOnHisFuture           0    0          1         1             0       0   \n",
       "SainzOnHisFuture           0    0          0         0             0       0   \n",
       "TotoOnMercedes             0    0          0         0             1       0   \n",
       "TsunodaPoints              1    1          0         0             0       1   \n",
       "\n",
       "                        added  admits  admitted  advantage  ...  worse  worst  \\\n",
       "AlonsoPraisesAMR            1       0         0          0  ...      0      0   \n",
       "HamiltonsCarIssues          1       0         0          0  ...      0      0   \n",
       "LeclercsPoorQualifying      0       0         1          1  ...      0      0   \n",
       "PerezOnHisFuture            0       0         1          0  ...      1      1   \n",
       "SainzOnHisFuture            0       0         0          0  ...      0      0   \n",
       "TotoOnMercedes              1       1         1          0  ...      0      0   \n",
       "TsunodaPoints               0       0         0          0  ...      0      0   \n",
       "\n",
       "                        worth  yeah  year  yesterday  youll  youve  yuki  zone  \n",
       "AlonsoPraisesAMR            0     1     0          0      0      0     0     0  \n",
       "HamiltonsCarIssues          0     1     0          1      1      1     0     0  \n",
       "LeclercsPoorQualifying      0     0     0          2      0      0     0     0  \n",
       "PerezOnHisFuture            0     0     6          0      0      0     0     0  \n",
       "SainzOnHisFuture            1     1     5          0      0      0     0     0  \n",
       "TotoOnMercedes              0     0     1          0      0      0     0     0  \n",
       "TsunodaPoints               0     2     0          0      0      0     1     1  \n",
       "\n",
       "[7 rows x 802 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AlonsoPraisesAMR</th>\n",
       "      <th>HamiltonsCarIssues</th>\n",
       "      <th>LeclercsPoorQualifying</th>\n",
       "      <th>PerezOnHisFuture</th>\n",
       "      <th>SainzOnHisFuture</th>\n",
       "      <th>TotoOnMercedes</th>\n",
       "      <th>TsunodaPoints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abu</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>according</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>achilles</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acknowledged</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              AlonsoPraisesAMR  HamiltonsCarIssues  LeclercsPoorQualifying  \\\n",
       "able                         0                   1                       0   \n",
       "abu                          0                   0                       0   \n",
       "according                    0                   0                       0   \n",
       "achilles                     0                   0                       0   \n",
       "acknowledged                 1                   1                       0   \n",
       "\n",
       "              PerezOnHisFuture  SainzOnHisFuture  TotoOnMercedes  \\\n",
       "able                         0                 0               0   \n",
       "abu                          0                 0               0   \n",
       "according                    1                 0               0   \n",
       "achilles                     1                 0               0   \n",
       "acknowledged                 0                 0               1   \n",
       "\n",
       "              TsunodaPoints  \n",
       "able                      1  \n",
       "abu                       1  \n",
       "according                 0  \n",
       "achilles                  0  \n",
       "acknowledged              0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"red\" + 0.010*\"bull\" + 0.009*\"team\" + 0.009*\"russell\" + 0.008*\"perez\" + 0.008*\"mercedes\" + 0.008*\"hamilton\" + 0.008*\"car\" + 0.008*\"wolff\" + 0.007*\"better\"'),\n",
       " (1,\n",
       "  '0.009*\"sainz\" + 0.008*\"japan\" + 0.008*\"time\" + 0.008*\"good\" + 0.007*\"team\" + 0.007*\"grand\" + 0.007*\"prix\" + 0.007*\"think\" + 0.007*\"need\" + 0.007*\"really\"')]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"sainz\" + 0.012*\"future\" + 0.010*\"think\" + 0.010*\"time\" + 0.010*\"year\" + 0.008*\"obviously\" + 0.006*\"japan\" + 0.006*\"victory\" + 0.006*\"melbourne\" + 0.006*\"ferrari\"'),\n",
       " (1,\n",
       "  '0.013*\"red\" + 0.012*\"bull\" + 0.010*\"really\" + 0.010*\"perez\" + 0.009*\"qualifying\" + 0.009*\"good\" + 0.008*\"alonso\" + 0.008*\"team\" + 0.008*\"japan\" + 0.008*\"leclerc\"'),\n",
       " (2,\n",
       "  '0.011*\"team\" + 0.009*\"russell\" + 0.009*\"car\" + 0.009*\"hamilton\" + 0.008*\"prix\" + 0.008*\"grand\" + 0.008*\"wolff\" + 0.008*\"mercedes\" + 0.008*\"tsunoda\" + 0.008*\"better\"')]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"red\" + 0.021*\"bull\" + 0.019*\"perez\" + 0.013*\"year\" + 0.011*\"future\" + 0.011*\"season\" + 0.009*\"market\" + 0.009*\"mexican\" + 0.009*\"drivers\" + 0.007*\"sainz\"'),\n",
       " (1,\n",
       "  '0.012*\"team\" + 0.010*\"need\" + 0.009*\"russell\" + 0.009*\"prix\" + 0.009*\"japanese\" + 0.009*\"grand\" + 0.009*\"car\" + 0.008*\"better\" + 0.008*\"currently\" + 0.007*\"hamilton\"'),\n",
       " (2,\n",
       "  '0.019*\"sainz\" + 0.013*\"leclerc\" + 0.012*\"good\" + 0.012*\"really\" + 0.010*\"think\" + 0.010*\"time\" + 0.010*\"qualifying\" + 0.009*\"future\" + 0.007*\"japan\" + 0.007*\"year\"'),\n",
       " (3,\n",
       "  '0.001*\"team\" + 0.001*\"grand\" + 0.001*\"prix\" + 0.001*\"sainz\" + 0.001*\"good\" + 0.001*\"japan\" + 0.001*\"better\" + 0.001*\"need\" + 0.001*\"really\" + 0.001*\"think\"')]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #2 (Nouns Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AlonsoPraisesAMR</th>\n",
       "      <td>fernando alonso was delighted with his drive t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HamiltonsCarIssues</th>\n",
       "      <td>lewis hamilton was left to rue the struggles h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeclercsPoorQualifying</th>\n",
       "      <td>charles leclerc enjoyed a strong drive to  in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PerezOnHisFuture</th>\n",
       "      <td>sergio perez expects his red bull and  future ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SainzOnHisFuture</th>\n",
       "      <td>carlos sainz feels that now is the time to spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotoOnMercedes</th>\n",
       "      <td>toto wolff has admitted that mercedes are in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TsunodaPoints</th>\n",
       "      <td>yuki tsunoda was full of relief after scoring ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               transcript\n",
       "AlonsoPraisesAMR        fernando alonso was delighted with his drive t...\n",
       "HamiltonsCarIssues      lewis hamilton was left to rue the struggles h...\n",
       "LeclercsPoorQualifying  charles leclerc enjoyed a strong drive to  in ...\n",
       "PerezOnHisFuture        sergio perez expects his red bull and  future ...\n",
       "SainzOnHisFuture        carlos sainz feels that now is the time to spe...\n",
       "TotoOnMercedes          toto wolff has admitted that mercedes are in a...\n",
       "TsunodaPoints           yuki tsunoda was full of relief after scoring ..."
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anady\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\anady\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AlonsoPraisesAMR</th>\n",
       "      <td>fernando alonso drive prix – spaniard aston ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HamiltonsCarIssues</th>\n",
       "      <td>hamilton struggles mercedes prix world champio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeclercsPoorQualifying</th>\n",
       "      <td>charles drive prix monegasque price struggles ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PerezOnHisFuture</th>\n",
       "      <td>sergio perez bull future month contract year m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SainzOnHisFuture</th>\n",
       "      <td>carlos sainz feels time negotiations future op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotoOnMercedes</th>\n",
       "      <td>toto wolff mercedes process feet concept car s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TsunodaPoints</th>\n",
       "      <td>yuki tsunoda relief point home soil japan time...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               transcript\n",
       "AlonsoPraisesAMR        fernando alonso drive prix – spaniard aston ma...\n",
       "HamiltonsCarIssues      hamilton struggles mercedes prix world champio...\n",
       "LeclercsPoorQualifying  charles drive prix monegasque price struggles ...\n",
       "PerezOnHisFuture        sergio perez bull future month contract year m...\n",
       "SainzOnHisFuture        carlos sainz feels time negotiations future op...\n",
       "TotoOnMercedes          toto wolff mercedes process feet concept car s...\n",
       "TsunodaPoints           yuki tsunoda relief point home soil japan time..."
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abu</th>\n",
       "      <th>achilles</th>\n",
       "      <th>action</th>\n",
       "      <th>admits</th>\n",
       "      <th>advantage</th>\n",
       "      <th>agents</th>\n",
       "      <th>albon</th>\n",
       "      <th>alonso</th>\n",
       "      <th>ambition</th>\n",
       "      <th>announcements</th>\n",
       "      <th>...</th>\n",
       "      <th>wins</th>\n",
       "      <th>wolff</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>youll</th>\n",
       "      <th>youve</th>\n",
       "      <th>yuki</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AlonsoPraisesAMR</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HamiltonsCarIssues</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeclercsPoorQualifying</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PerezOnHisFuture</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SainzOnHisFuture</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotoOnMercedes</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TsunodaPoints</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 392 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        abu  achilles  action  admits  advantage  agents  \\\n",
       "AlonsoPraisesAMR          0         0       1       0          0       0   \n",
       "HamiltonsCarIssues        0         0       0       0          0       0   \n",
       "LeclercsPoorQualifying    0         0       0       0          1       0   \n",
       "PerezOnHisFuture          0         1       0       0          0       1   \n",
       "SainzOnHisFuture          0         0       0       0          0       0   \n",
       "TotoOnMercedes            0         0       0       1          0       0   \n",
       "TsunodaPoints             1         0       1       0          0       0   \n",
       "\n",
       "                        albon  alonso  ambition  announcements  ...  wins  \\\n",
       "AlonsoPraisesAMR            0       5         0              0  ...     1   \n",
       "HamiltonsCarIssues          1       0         0              0  ...     0   \n",
       "LeclercsPoorQualifying      0       0         0              0  ...     0   \n",
       "PerezOnHisFuture            0       1         1              1  ...     0   \n",
       "SainzOnHisFuture            0       0         0              0  ...     0   \n",
       "TotoOnMercedes              0       0         0              0  ...     0   \n",
       "TsunodaPoints               1       0         0              0  ...     1   \n",
       "\n",
       "                        wolff  work  world  year  yesterday  youll  youve  \\\n",
       "AlonsoPraisesAMR            0     0      1     0          0      0      0   \n",
       "HamiltonsCarIssues          0     1      1     0          1      1      1   \n",
       "LeclercsPoorQualifying      0     1      0     0          2      0      0   \n",
       "PerezOnHisFuture            0     0      0     6          0      0      0   \n",
       "SainzOnHisFuture            0     0      0     5          0      0      0   \n",
       "TotoOnMercedes              6     0      0     1          0      0      0   \n",
       "TsunodaPoints               0     1      0     0          0      0      0   \n",
       "\n",
       "                        yuki  zone  \n",
       "AlonsoPraisesAMR           0     0  \n",
       "HamiltonsCarIssues         0     0  \n",
       "LeclercsPoorQualifying     0     0  \n",
       "PerezOnHisFuture           0     0  \n",
       "SainzOnHisFuture           0     0  \n",
       "TotoOnMercedes             0     0  \n",
       "TsunodaPoints              1     1  \n",
       "\n",
       "[7 rows x 392 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = list(text.ENGLISH_STOP_WORDS.union(add_stop_words))\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names_out())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.033*\"race\" + 0.018*\"team\" + 0.017*\"car\" + 0.017*\"prix\" + 0.014*\"hamilton\" + 0.014*\"mercedes\" + 0.013*\"weekend\" + 0.013*\"japan\" + 0.013*\"russell\" + 0.011*\"pace\"'),\n",
       " (1,\n",
       "  '0.021*\"sainz\" + 0.018*\"bull\" + 0.018*\"year\" + 0.016*\"future\" + 0.015*\"team\" + 0.013*\"race\" + 0.013*\"japan\" + 0.013*\"season\" + 0.013*\"tsunoda\" + 0.010*\"points\"')]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.027*\"race\" + 0.025*\"car\" + 0.025*\"hamilton\" + 0.022*\"mercedes\" + 0.020*\"team\" + 0.020*\"russell\" + 0.015*\"prix\" + 0.015*\"wolff\" + 0.013*\"weekend\" + 0.013*\"stint\"'),\n",
       " (1,\n",
       "  '0.030*\"sainz\" + 0.028*\"bull\" + 0.028*\"year\" + 0.025*\"future\" + 0.016*\"season\" + 0.016*\"perez\" + 0.013*\"japan\" + 0.011*\"race\" + 0.011*\"team\" + 0.011*\"victory\"'),\n",
       " (2,\n",
       "  '0.030*\"race\" + 0.018*\"team\" + 0.016*\"japan\" + 0.016*\"prix\" + 0.013*\"pace\" + 0.013*\"tsunoda\" + 0.013*\"points\" + 0.011*\"cookies\" + 0.011*\"today\" + 0.011*\"feature\"')]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"bull\" + 0.003*\"season\" + 0.003*\"perez\" + 0.003*\"year\" + 0.003*\"team\" + 0.003*\"future\" + 0.003*\"race\" + 0.003*\"drivers\" + 0.003*\"japan\" + 0.003*\"rb\"'),\n",
       " (1,\n",
       "  '0.021*\"race\" + 0.019*\"team\" + 0.019*\"bull\" + 0.019*\"sainz\" + 0.018*\"japan\" + 0.016*\"year\" + 0.015*\"future\" + 0.012*\"prix\" + 0.012*\"tsunoda\" + 0.012*\"season\"'),\n",
       " (2,\n",
       "  '0.035*\"race\" + 0.021*\"prix\" + 0.018*\"team\" + 0.018*\"wolff\" + 0.015*\"mercedes\" + 0.015*\"day\" + 0.012*\"car\" + 0.012*\"hamilton\" + 0.012*\"japan\" + 0.012*\"today\"'),\n",
       " (3,\n",
       "  '0.025*\"race\" + 0.025*\"car\" + 0.025*\"hamilton\" + 0.021*\"russell\" + 0.017*\"mercedes\" + 0.013*\"team\" + 0.013*\"suzuka\" + 0.013*\"season\" + 0.013*\"george\" + 0.013*\"circuits\"')]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AlonsoPraisesAMR</th>\n",
       "      <td>fernando alonso drive japanese grand prix – sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HamiltonsCarIssues</th>\n",
       "      <td>lewis hamilton struggles mercedes japanese gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeclercsPoorQualifying</th>\n",
       "      <td>charles strong drive japanese grand prix moneg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PerezOnHisFuture</th>\n",
       "      <td>sergio perez red bull future next month mexica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SainzOnHisFuture</th>\n",
       "      <td>carlos sainz feels time negotiations future op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotoOnMercedes</th>\n",
       "      <td>toto wolff mercedes process live feet concept ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TsunodaPoints</th>\n",
       "      <td>yuki tsunoda full relief point home soil japan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               transcript\n",
       "AlonsoPraisesAMR        fernando alonso drive japanese grand prix – sp...\n",
       "HamiltonsCarIssues      lewis hamilton struggles mercedes japanese gra...\n",
       "LeclercsPoorQualifying  charles strong drive japanese grand prix moneg...\n",
       "PerezOnHisFuture        sergio perez red bull future next month mexica...\n",
       "SainzOnHisFuture        carlos sainz feels time negotiations future op...\n",
       "TotoOnMercedes          toto wolff mercedes process live feet concept ...\n",
       "TsunodaPoints           yuki tsunoda full relief point home soil japan..."
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>abu</th>\n",
       "      <th>achilles</th>\n",
       "      <th>action</th>\n",
       "      <th>admits</th>\n",
       "      <th>advantage</th>\n",
       "      <th>agents</th>\n",
       "      <th>albon</th>\n",
       "      <th>alex</th>\n",
       "      <th>alonso</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>worse</th>\n",
       "      <th>worst</th>\n",
       "      <th>worth</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>youll</th>\n",
       "      <th>youve</th>\n",
       "      <th>yuki</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AlonsoPraisesAMR</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HamiltonsCarIssues</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeclercsPoorQualifying</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PerezOnHisFuture</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SainzOnHisFuture</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotoOnMercedes</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TsunodaPoints</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        able  abu  achilles  action  admits  advantage  \\\n",
       "AlonsoPraisesAMR           0    0         0       1       0          0   \n",
       "HamiltonsCarIssues         1    0         0       0       0          0   \n",
       "LeclercsPoorQualifying     0    0         0       0       0          1   \n",
       "PerezOnHisFuture           0    0         1       0       0          0   \n",
       "SainzOnHisFuture           0    0         0       0       0          0   \n",
       "TotoOnMercedes             0    0         0       0       1          0   \n",
       "TsunodaPoints              1    1         0       1       0          0   \n",
       "\n",
       "                        agents  albon  alex  alonso  ...  world  worse  worst  \\\n",
       "AlonsoPraisesAMR             0      0     0       5  ...      1      0      0   \n",
       "HamiltonsCarIssues           0      1     1       0  ...      1      0      0   \n",
       "LeclercsPoorQualifying       0      0     0       0  ...      0      0      0   \n",
       "PerezOnHisFuture             1      0     0       1  ...      0      1      1   \n",
       "SainzOnHisFuture             0      0     0       0  ...      0      0      0   \n",
       "TotoOnMercedes               0      0     0       0  ...      0      0      0   \n",
       "TsunodaPoints                0      1     0       0  ...      0      0      0   \n",
       "\n",
       "                        worth  year  yesterday  youll  youve  yuki  zone  \n",
       "AlonsoPraisesAMR            0     0          0      0      0     0     0  \n",
       "HamiltonsCarIssues          0     0          1      1      1     0     0  \n",
       "LeclercsPoorQualifying      0     0          2      0      0     0     0  \n",
       "PerezOnHisFuture            0     6          0      0      0     0     0  \n",
       "SainzOnHisFuture            1     5          0      0      0     0     0  \n",
       "TotoOnMercedes              0     1          0      0      0     0     0  \n",
       "TsunodaPoints               0     0          0      0      0     1     1  \n",
       "\n",
       "[7 rows x 512 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names_out())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.023*\"sainz\" + 0.014*\"leclerc\" + 0.012*\"future\" + 0.010*\"year\" + 0.010*\"today\" + 0.008*\"ferrari\" + 0.008*\"past\" + 0.008*\"bit\" + 0.008*\"day\" + 0.006*\"victory\"'),\n",
       " (1,\n",
       "  '0.013*\"bull\" + 0.011*\"japanese\" + 0.011*\"car\" + 0.010*\"mercedes\" + 0.009*\"perez\" + 0.009*\"season\" + 0.009*\"weekend\" + 0.009*\"hamilton\" + 0.008*\"available\" + 0.008*\"tsunoda\"')]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"car\" + 0.016*\"hamilton\" + 0.014*\"mercedes\" + 0.013*\"russell\" + 0.011*\"japanese\" + 0.011*\"wolff\" + 0.010*\"leclerc\" + 0.010*\"day\" + 0.010*\"better\" + 0.008*\"early\"'),\n",
       " (1,\n",
       "  '0.020*\"bull\" + 0.019*\"sainz\" + 0.017*\"year\" + 0.017*\"future\" + 0.014*\"perez\" + 0.011*\"weekend\" + 0.010*\"pace\" + 0.010*\"alonso\" + 0.010*\"season\" + 0.008*\"verstappen\"'),\n",
       " (2,\n",
       "  '0.021*\"tsunoda\" + 0.016*\"points\" + 0.011*\"point\" + 0.011*\"home\" + 0.011*\"pit\" + 0.011*\"cars\" + 0.011*\"start\" + 0.011*\"japanese\" + 0.009*\"rb\" + 0.009*\"stop\"')]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.026*\"wolff\" + 0.019*\"mercedes\" + 0.015*\"car\" + 0.015*\"hamilton\" + 0.012*\"weekend\" + 0.012*\"russell\" + 0.012*\"better\" + 0.012*\"stint\" + 0.012*\"direction\" + 0.012*\"experiments\"'),\n",
       " (1,\n",
       "  '0.013*\"pace\" + 0.013*\"japanese\" + 0.013*\"car\" + 0.010*\"hamilton\" + 0.010*\"available\" + 0.010*\"today\" + 0.010*\"weekend\" + 0.010*\"leclerc\" + 0.010*\"russell\" + 0.010*\"early\"'),\n",
       " (2,\n",
       "  '0.034*\"sainz\" + 0.023*\"future\" + 0.019*\"year\" + 0.012*\"victory\" + 0.012*\"melbourne\" + 0.012*\"ferrari\" + 0.012*\"challenging\" + 0.012*\"news\" + 0.012*\"australia\" + 0.012*\"options\"'),\n",
       " (3,\n",
       "  '0.019*\"bull\" + 0.017*\"tsunoda\" + 0.016*\"perez\" + 0.014*\"season\" + 0.012*\"year\" + 0.012*\"point\" + 0.012*\"points\" + 0.010*\"future\" + 0.010*\"home\" + 0.010*\"rb\"')]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.030*\"sainz\" + 0.018*\"leclerc\" + 0.015*\"future\" + 0.013*\"today\" + 0.013*\"year\" + 0.010*\"bit\" + 0.010*\"past\" + 0.010*\"day\" + 0.010*\"ferrari\" + 0.008*\"challenging\"'),\n",
       " (1,\n",
       "  '0.002*\"best\" + 0.002*\"soft\" + 0.002*\"tyres\" + 0.002*\"result\" + 0.002*\"bit\" + 0.002*\"multiple\" + 0.002*\"work\" + 0.002*\"overtakes\" + 0.002*\"moment\" + 0.002*\"happy\"'),\n",
       " (2,\n",
       "  '0.024*\"tsunoda\" + 0.018*\"points\" + 0.012*\"home\" + 0.012*\"japanese\" + 0.012*\"start\" + 0.012*\"pit\" + 0.012*\"cars\" + 0.012*\"point\" + 0.009*\"feature\" + 0.009*\"preferences\"'),\n",
       " (3,\n",
       "  '0.018*\"bull\" + 0.015*\"car\" + 0.014*\"mercedes\" + 0.013*\"perez\" + 0.013*\"hamilton\" + 0.013*\"weekend\" + 0.012*\"russell\" + 0.010*\"season\" + 0.010*\"japanese\" + 0.009*\"verstappen\"')]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics arent looking great, we might have to alter the parameters further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'AlonsoPraisesAMR'),\n",
       " (3, 'HamiltonsCarIssues'),\n",
       " (0, 'LeclercsPoorQualifying'),\n",
       " (3, 'PerezOnHisFuture'),\n",
       " (0, 'SainzOnHisFuture'),\n",
       " (3, 'TotoOnMercedes'),\n",
       " (2, 'TsunodaPoints')]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "topics = [a for doc in corpus_transformed for a, _ in doc]\n",
    "list(zip(topics, data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment:\n",
    "1. Try further modifying the parameters of the topic models above and see if you can get better topics.\n",
    "2. Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and see if you can get better topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.024*\"tsunoda\" + 0.018*\"points\" + 0.012*\"start\" + 0.012*\"point\" + 0.012*\"cars\" + 0.012*\"home\" + 0.012*\"japanese\" + 0.012*\"pit\" + 0.009*\"stop\" + 0.009*\"feature\"'),\n",
       " (1,\n",
       "  '0.026*\"bull\" + 0.020*\"perez\" + 0.013*\"pace\" + 0.013*\"year\" + 0.013*\"alonso\" + 0.011*\"season\" + 0.011*\"verstappen\" + 0.011*\"weekend\" + 0.011*\"future\" + 0.009*\"driver\"'),\n",
       " (2,\n",
       "  '0.034*\"sainz\" + 0.023*\"future\" + 0.019*\"year\" + 0.012*\"ferrari\" + 0.012*\"victory\" + 0.012*\"challenging\" + 0.012*\"melbourne\" + 0.012*\"news\" + 0.012*\"australia\" + 0.012*\"options\"'),\n",
       " (3,\n",
       "  '0.017*\"car\" + 0.017*\"hamilton\" + 0.015*\"mercedes\" + 0.014*\"russell\" + 0.012*\"wolff\" + 0.012*\"japanese\" + 0.010*\"better\" + 0.010*\"day\" + 0.010*\"leclerc\" + 0.009*\"today\"')]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=87)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: Team Ferrari\n",
    "* Topic 1: Team Mercedes\n",
    "* Topic 2: Team RedBull\n",
    "* Topic 3: Japan Race Pace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'AlonsoPraisesAMR'),\n",
       " (1, 'HamiltonsCarIssues'),\n",
       " (0, 'LeclercsPoorQualifying'),\n",
       " (2, 'PerezOnHisFuture'),\n",
       " (0, 'SainzOnHisFuture'),\n",
       " (1, 'TotoOnMercedes'),\n",
       " (3, 'TsunodaPoints')]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "topics = [a for doc in corpus_transformed for a, _ in doc]\n",
    "list(zip(topics, data_dtmna.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.025*\"tsunoda\" + 0.019*\"points\" + 0.013*\"home\" + 0.013*\"japanese\" + 0.013*\"pit\" + 0.013*\"cars\" + 0.013*\"point\" + 0.013*\"start\" + 0.010*\"feature\" + 0.010*\"cookies\"'),\n",
       " (1,\n",
       "  '0.017*\"weekend\" + 0.017*\"wolff\" + 0.015*\"pace\" + 0.015*\"mercedes\" + 0.015*\"car\" + 0.012*\"japanese\" + 0.012*\"alonso\" + 0.010*\"available\" + 0.010*\"better\" + 0.010*\"stroll\"'),\n",
       " (2,\n",
       "  '0.032*\"sainz\" + 0.019*\"leclerc\" + 0.016*\"future\" + 0.013*\"today\" + 0.013*\"year\" + 0.011*\"day\" + 0.011*\"bit\" + 0.011*\"past\" + 0.011*\"ferrari\" + 0.008*\"place\"'),\n",
       " (3,\n",
       "  '0.036*\"bull\" + 0.029*\"perez\" + 0.022*\"year\" + 0.018*\"season\" + 0.018*\"future\" + 0.015*\"market\" + 0.015*\"drivers\" + 0.015*\"mexican\" + 0.011*\"verstappen\" + 0.011*\"sainz\"'),\n",
       " (4,\n",
       "  '0.022*\"hamilton\" + 0.022*\"car\" + 0.018*\"russell\" + 0.015*\"mercedes\" + 0.011*\"early\" + 0.011*\"japanese\" + 0.011*\"george\" + 0.011*\"season\" + 0.011*\"circuits\" + 0.011*\"tracks\"')]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=25)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'AlonsoPraisesAMR'),\n",
       " (4, 'HamiltonsCarIssues'),\n",
       " (2, 'LeclercsPoorQualifying'),\n",
       " (3, 'PerezOnHisFuture'),\n",
       " (2, 'SainzOnHisFuture'),\n",
       " (1, 'TotoOnMercedes'),\n",
       " (0, 'TsunodaPoints')]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "topics = [a for doc in corpus_transformed for a, _ in doc]\n",
    "list(zip(topics, data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 topics isn't working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out proper nouns from a string of text\n",
    "def prop_nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the proper nouns'''\n",
    "    is_prop_noun = lambda pos: pos[:2] == 'VB' or pos[:2] == 'NN' or pos[:2] == 'JJ' or pos[:2] == 'RB'\n",
    "    tokenized = word_tokenize(text)\n",
    "    prop_nouns = [word for (word, pos) in pos_tag(tokenized) if is_prop_noun(pos)] \n",
    "    return ' '.join(prop_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AlonsoPraisesAMR</th>\n",
       "      <td>fernando alonso was delighted drive japanese g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HamiltonsCarIssues</th>\n",
       "      <td>lewis hamilton was left rue struggles faced me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeclercsPoorQualifying</th>\n",
       "      <td>charles leclerc enjoyed strong drive japanese ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PerezOnHisFuture</th>\n",
       "      <td>sergio perez expects red bull future be clarif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SainzOnHisFuture</th>\n",
       "      <td>carlos sainz feels now is time speed negotiati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotoOnMercedes</th>\n",
       "      <td>toto wolff has admitted mercedes are process l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TsunodaPoints</th>\n",
       "      <td>yuki tsunoda was full relief scoring point hom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               transcript\n",
       "AlonsoPraisesAMR        fernando alonso was delighted drive japanese g...\n",
       "HamiltonsCarIssues      lewis hamilton was left rue struggles faced me...\n",
       "LeclercsPoorQualifying  charles leclerc enjoyed strong drive japanese ...\n",
       "PerezOnHisFuture        sergio perez expects red bull future be clarif...\n",
       "SainzOnHisFuture        carlos sainz feels now is time speed negotiati...\n",
       "TotoOnMercedes          toto wolff has admitted mercedes are process l...\n",
       "TsunodaPoints           yuki tsunoda was full relief scoring point hom..."
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the porp_nouns function to the transcripts to filter only on proper nouns\n",
    "data_prop_nouns = pd.DataFrame(data_clean.transcript.apply(prop_nouns))\n",
    "data_prop_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>abu</th>\n",
       "      <th>according</th>\n",
       "      <th>achilles</th>\n",
       "      <th>acknowledged</th>\n",
       "      <th>action</th>\n",
       "      <th>added</th>\n",
       "      <th>admits</th>\n",
       "      <th>admitted</th>\n",
       "      <th>advantage</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>worse</th>\n",
       "      <th>worst</th>\n",
       "      <th>worth</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>youll</th>\n",
       "      <th>youve</th>\n",
       "      <th>yuki</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AlonsoPraisesAMR</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HamiltonsCarIssues</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeclercsPoorQualifying</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PerezOnHisFuture</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SainzOnHisFuture</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotoOnMercedes</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TsunodaPoints</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        able  abu  according  achilles  acknowledged  action  \\\n",
       "AlonsoPraisesAMR           0    0          0         0             1       1   \n",
       "HamiltonsCarIssues         1    0          0         0             1       0   \n",
       "LeclercsPoorQualifying     0    0          0         0             0       0   \n",
       "PerezOnHisFuture           0    0          1         1             0       0   \n",
       "SainzOnHisFuture           0    0          0         0             0       0   \n",
       "TotoOnMercedes             0    0          0         0             1       0   \n",
       "TsunodaPoints              1    1          0         0             0       1   \n",
       "\n",
       "                        added  admits  admitted  advantage  ...  world  worse  \\\n",
       "AlonsoPraisesAMR            1       0         0          0  ...      1      0   \n",
       "HamiltonsCarIssues          1       0         0          0  ...      1      0   \n",
       "LeclercsPoorQualifying      0       0         1          1  ...      0      0   \n",
       "PerezOnHisFuture            0       0         1          0  ...      0      1   \n",
       "SainzOnHisFuture            0       0         0          0  ...      0      0   \n",
       "TotoOnMercedes              1       1         1          0  ...      0      0   \n",
       "TsunodaPoints               0       0         0          0  ...      0      0   \n",
       "\n",
       "                        worst  worth  year  yesterday  youll  youve  yuki  \\\n",
       "AlonsoPraisesAMR            0      0     0          0      0      0     0   \n",
       "HamiltonsCarIssues          0      0     0          1      1      1     0   \n",
       "LeclercsPoorQualifying      0      0     0          2      0      0     0   \n",
       "PerezOnHisFuture            1      0     6          0      0      0     0   \n",
       "SainzOnHisFuture            0      1     5          0      0      0     0   \n",
       "TotoOnMercedes              0      0     1          0      0      0     0   \n",
       "TsunodaPoints               0      0     0          0      0      0     1   \n",
       "\n",
       "                        zone  \n",
       "AlonsoPraisesAMR           0  \n",
       "HamiltonsCarIssues         0  \n",
       "LeclercsPoorQualifying     0  \n",
       "PerezOnHisFuture           0  \n",
       "SainzOnHisFuture           0  \n",
       "TotoOnMercedes             0  \n",
       "TsunodaPoints              1  \n",
       "\n",
       "[7 rows x 772 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only proper nouns, also remove common words with max_df\n",
    "cvpn = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvpn = cvpn.fit_transform(data_prop_nouns.transcript)\n",
    "data_dtmpn = pd.DataFrame(data_cvpn.toarray(), columns=cvpn.get_feature_names_out())\n",
    "data_dtmpn.index = data_prop_nouns.index\n",
    "data_dtmpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpuspn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmpn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordpn = dict((v, k) for k, v in cvpn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"russell\" + 0.014*\"car\" + 0.014*\"hamilton\" + 0.009*\"let\" + 0.009*\"mercedes\" + 0.007*\"early\" + 0.007*\"currently\" + 0.007*\"japanese\" + 0.007*\"george\" + 0.007*\"season\"'),\n",
       " (1,\n",
       "  '0.021*\"sainz\" + 0.015*\"leclerc\" + 0.013*\"really\" + 0.011*\"qualifying\" + 0.010*\"future\" + 0.008*\"year\" + 0.008*\"ferrari\" + 0.008*\"today\" + 0.008*\"bit\" + 0.007*\"day\"'),\n",
       " (2,\n",
       "  '0.012*\"bull\" + 0.010*\"perez\" + 0.010*\"tsunoda\" + 0.009*\"alonso\" + 0.008*\"need\" + 0.008*\"pit\" + 0.007*\"season\" + 0.007*\"stop\" + 0.007*\"points\" + 0.007*\"japanese\"'),\n",
       " (3,\n",
       "  '0.019*\"wolff\" + 0.012*\"mercedes\" + 0.012*\"weve\" + 0.010*\"car\" + 0.010*\"hamilton\" + 0.008*\"need\" + 0.008*\"weekend\" + 0.008*\"having\" + 0.008*\"stint\" + 0.008*\"russell\"')]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldapn = models.LdaModel(corpus=corpuspn, num_topics=4, id2word=id2wordpn, passes=25)\n",
    "ldapn.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'AlonsoPraisesAMR'),\n",
       " (4, 'HamiltonsCarIssues'),\n",
       " (2, 'LeclercsPoorQualifying'),\n",
       " (3, 'PerezOnHisFuture'),\n",
       " (2, 'SainzOnHisFuture'),\n",
       " (1, 'TotoOnMercedes'),\n",
       " (0, 'TsunodaPoints')]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "topics = [a for doc in corpus_transformed for a, _ in doc]\n",
    "corpus_transformed = ldapn[corpuspn]\n",
    "list(zip(topics, data_dtmpn.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results are shown with 4 topics using only nouns and adjectives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
